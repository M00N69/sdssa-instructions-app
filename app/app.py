import streamlit as st
import pandas as pd
import sqlite3
import os
import requests
from bs4 import BeautifulSoup
from whoosh.index import create_in
from whoosh.fields import Schema, TEXT, ID
from whoosh.qparser import QueryParser
from whoosh.analysis import StemmingAnalyzer, LowercaseFilter, StopFilter
import nltk
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from datetime import datetime, timedelta

# Configuration de la page
st.set_page_config(layout="wide")

# Ex√©cuter le script d'initialisation NLTK
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Fonction pour v√©rifier si la base de donn√©es existe
def check_database():
    db_path = 'data/sdssa_instructions.db'
    if not os.path.exists(db_path):
        st.error(f"La base de donn√©es {db_path} n'existe pas. Veuillez v√©rifier le chemin et essayer √† nouveau.")
        st.stop()
    return db_path

# Fonction pour lire les donn√©es depuis la base de donn√©es SQLite
def load_data(db_path):
    conn = sqlite3.connect(db_path)
    query = "SELECT * FROM instructions"
    df = pd.read_sql_query(query, conn)
    conn.close()
    return df

# Fonction pour cr√©er un index Whoosh avec des analyses avanc√©es
def create_whoosh_index(df):
    analyzer = StemmingAnalyzer() | LowercaseFilter() | StopFilter()
    schema = Schema(title=TEXT(stored=True, analyzer=analyzer),
                    objet=TEXT(stored=True, analyzer=analyzer),
                    resume=TEXT(stored=True, analyzer=analyzer),
                    content=TEXT(analyzer=analyzer))
    if not os.path.exists("indexdir"):
        os.mkdir("indexdir")
    ix = create_in("indexdir", schema)
    writer = ix.writer()
    for index, row in df.iterrows():
        writer.add_document(title=row['title'], objet=row['objet'], resume=row['resume'], content=f"{row['title']} {row['objet']} {row['resume']}")
    writer.commit()
    return ix

# Fonction pour trouver des synonymes
def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for lemma in syn.lemmas():
            synonyms.add(lemma.name().lower())
    return synonyms

# Fonction pour normaliser le texte
def normalize_text(text):
    lemmatizer = WordNetLemmatizer()
    words = word_tokenize(text.lower())
    normalized_words = [lemmatizer.lemmatize(word) for word in words]
    return ' '.join(normalized_words)

# Fonction pour r√©cup√©rer les nouvelles instructions des semaines manquantes
def get_new_instructions(year, week):
    url = f"https://info.agriculture.gouv.fr/boagri/historique/annee-{year}/semaine-{week}"
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        instructions = soup.find_all('a', href=True)
        sdssa_instructions = [a for a in instructions if 'SDSSA' in a.text]
        new_instructions = []
        for instruction in sdssa_instructions:
            href = instruction['href']
            if not href.startswith(('http://', 'https://')):
                href = f"https://info.agriculture.gouv.fr{href}"
            link = href
            pdf_link = link.replace("/detail", "/telechargement")
            new_instructions.append((instruction.text, link, pdf_link))
        return new_instructions
    else:
        print(f"Failed to retrieve data for year {year} week {week}")
        return []

# Fonction pour ajouter une instruction √† la base de donn√©es
def add_instruction_to_db(year, week, title, link, pdf_link, objet, resume):
    conn = sqlite3.connect('data/sdssa_instructions.db')
    cursor = conn.cursor()
    try:
        cursor.execute("""
            INSERT INTO instructions (year, week, title, link, pdf_link, objet, resume, last_updated)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(title) DO UPDATE SET
            year=excluded.year,
            week=excluded.week,
            link=excluded.link,
            pdf_link=excluded.pdf_link,
            objet=excluded.objet,
            resume=excluded.resume,
            last_updated=excluded.last_updated;
        """, (year, week, title, link, pdf_link, objet, resume, datetime.now()))
        conn.commit()
    except sqlite3.Error as e:
        print(f"Error inserting data: {e}")
    finally:
        conn.close()

# V√©rifier la base de donn√©es
db_path = check_database()

# Charger les donn√©es
data = load_data(db_path)

# V√©rifier les colonnes attendues
required_columns = ['year', 'week', 'title', 'link', 'pdf_link', 'objet', 'resume']
missing_columns = [col for col in required_columns if col not in data.columns]
if missing_columns:
    st.error(f"Les colonnes suivantes sont manquantes dans la base de donn√©es : {', '.join(missing_columns)}")
    st.stop()

# Cr√©er un index Whoosh
ix = create_whoosh_index(data)

# Titre de l'application
st.title("Instructions Techniques DGAL / SDSSA")

# Instructions et explications
with st.expander("Instructions et explications d'utilisation"):
    st.markdown("""
    <div style="background-color: #f9f9f9; padding: 10px; border-radius: 5px;">
        <p>Bienvenue sur l'application SDSSA Instructions. Utilisez les filtres pour rechercher des instructions techniques par ann√©e, semaine, ou mots-cl√©s. Vous pouvez √©galement effectuer une recherche avanc√©e pour des r√©sultats plus pr√©cis.</p>
        <p>Pour t√©l√©charger les donn√©es, utilisez le bouton de t√©l√©chargement dans la barre lat√©rale.</p>
        <p><strong>Note :</strong> La recherche avanc√©e est prioritaire. Si vous utilisez la recherche avanc√©e, les filtres par ann√©e, semaine et mot-cl√© ne seront pas appliqu√©s.</p>
    </div>
    """, unsafe_allow_html=True)

# Recherche avanc√©e
st.sidebar.subheader("Recherche avanc√©e")
advanced_search = st.sidebar.text_input("Recherche avanc√©e")
st.sidebar.markdown("Utilisez la recherche avanc√©e pour inclure des synonymes et obtenir des r√©sultats plus pr√©cis.")

# Filtres par ann√©e et semaine dans un expander
with st.sidebar.expander("Filtrer par ann√©e et semaine"):
    years = data['year'].unique()
    weeks = data['week'].unique()
    all_weeks_option = "Toutes les semaines"
    weeks = sorted(set(weeks))
    weeks.insert(0, all_weeks_option)  # Ajouter l'option "Toutes les semaines" au d√©but
    year = st.selectbox("Ann√©e", years)
    week = st.selectbox("Semaine", weeks)

# Logo Visipilot
st.sidebar.markdown(
    """
    <div style="text-align: center; margin-top: 20px; width: 100%;">
        <a href="https://www.visipilot.com" target="_blank">
            <img src="https://github.com/M00N69/sdssa-instructions-app/blob/main/app/assets/logo.png?raw=true" alt="Visipilot Logo" style="width: 100%;">
        </a>
    </div>
    """,
    unsafe_allow_html=True
)

# Initialiser filtered_data avec toutes les donn√©es
filtered_data = data.copy()

# Appliquer les filtres
if advanced_search:
    # Normaliser la recherche avanc√©e
    normalized_search = normalize_text(advanced_search)
    # Trouver des synonymes
    synonyms = set()
    for word in word_tokenize(normalized_search):
        synonyms.update(get_synonyms(word))
    synonyms.add(normalized_search)

    # Cr√©er une requ√™te qui inclut les synonymes
    query_string = " OR ".join([f"content:{syn}" for syn in synonyms])
    with ix.searcher() as searcher:
        query = QueryParser("content", ix.schema).parse(query_string)
        results = searcher.search(query)
        filtered_data = pd.DataFrame([{
            'year': data.loc[data['title'] == hit['title'], 'year'].values[0],
            'week': data.loc[data['title'] == hit['title'], 'week'].values[0],
            'title': hit['title'],
            'link': data.loc[data['title'] == hit['title'], 'link'].values[0],
            'pdf_link': data.loc[data['title'] == hit['title'], 'pdf_link'].values[0],
            'objet': hit['objet'],
            'resume': hit['resume']
        } for hit in results])
else:
    # Filtrer les donn√©es selon les filtres d'ann√©e et de semaine
    if week != all_weeks_option:
        filtered_data = filtered_data[(filtered_data['year'] == year) & (filtered_data['week'] == week)]
    else:
        filtered_data = filtered_data[filtered_data['year'] == year]

# Afficher les r√©sultats filtr√©s
if filtered_data.empty:
    st.write("Aucun r√©sultat trouv√© avec les filtres actuels.")
else:
    st.write("R√©sultats filtr√©s :")
    st.dataframe(filtered_data[['objet', 'resume']])

    # S√©lection d'une instruction pour afficher les d√©tails
    st.header("D√©tails d'une instruction")
    selected_title = st.selectbox("S√©lectionner une instruction", filtered_data['title'])
    if selected_title:
        instruction_details = filtered_data[filtered_data['title'] == selected_title].iloc[0]
        st.markdown(f"### D√©tails de l'instruction : {selected_title}")
        st.markdown(f"**Ann√©e :** {instruction_details['year']}")
        st.markdown(f"**Semaine :** {instruction_details['week']}")
        st.markdown(f"**Objet :** {instruction_details['objet']}")
        st.markdown(f"**R√©sum√© :** {instruction_details['resume']}")
        st.markdown(f"**Lien :** [{instruction_details['title']}]({instruction_details['link']})")
        st.markdown(f"**T√©l√©charger le PDF :** [{instruction_details['title']}]({instruction_details['pdf_link']})")

# T√©l√©chargement des donn√©es
st.sidebar.header("T√©l√©charger les donn√©es")
if st.sidebar.button("T√©l√©charger le CSV"):
    if filtered_data.empty:
        st.sidebar.warning("Aucune donn√©e √† t√©l√©charger.")
    else:
        csv = filtered_data.to_csv(index=False).encode('utf-8')
        st.sidebar.download_button(
            label="T√©l√©charger",
            data=csv,
            file_name="sdssa_instructions.csv",
            mime="text/csv"
        )

# Bouton pour mettre √† jour les donn√©es
if st.sidebar.button("Mettre √† jour les donn√©es"):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    try:
        # üîç Trouver la derni√®re semaine enregistr√©e
        cursor.execute("SELECT MAX(year), MAX(week) FROM instructions;")
        latest_entry = cursor.fetchone()

        # Si la base est vide, on commence en 2019 semaine 1
        latest_year, latest_week = latest_entry if latest_entry != (None, None) else (2019, 1)

        current_year, current_week = datetime.now().isocalendar()[:2]

        # üìÖ Identifier les semaines √† v√©rifier (uniquement apr√®s la derni√®re semaine en base)
        weeks_to_check = []
        for year in range(latest_year, current_year + 1):
            start_week = latest_week + 1 if year == latest_year else 1
            end_week = current_week if year == current_year else 52
            for week in range(start_week, end_week + 1):
                weeks_to_check.append((year, week))

        st.write(f"üìÖ Semaines √† v√©rifier : {weeks_to_check}")

        # üì° R√©cup√©rer uniquement les nouvelles instructions
        new_instructions = []
        for year, week in weeks_to_check:
            instructions = get_new_instructions(year, week)
            for instruction in instructions:
                link = f"https://info.agriculture.gouv.fr{instruction['href']}"
                pdf_link = link.replace("/detail", "/telechargement")

                # R√©cup√©rer l'objet et le r√©sum√©
                response = requests.get(link)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    objet = soup.find('h1').text.strip() if soup.find('h1') else "OBJET : Inconnu"
                    resume = soup.find('p').text.strip() if soup.find('p') else "RESUME : Inconnu"
                else:
                    objet, resume = "OBJET : Inconnu", "RESUME : Inconnu"

                # üîç V√©rifier si cette instruction est d√©j√† en base
                cursor.execute("SELECT COUNT(*) FROM instructions WHERE title = ?", (instruction.text,))
                exists = cursor.fetchone()[0]

                if exists == 0:
                    new_instructions.append((year, week, instruction.text, link, pdf_link, objet, resume))

        st.write(f"üìÑ {len(new_instructions)} nouvelles instructions trouv√©es.")

        # ‚úÖ Ajouter les nouvelles instructions √† la base
        added_count = 0
        for instruction in new_instructions:
            year, week, title, link, pdf_link, objet, resume = instruction
            cursor.execute("""
                INSERT INTO instructions (year, week, title, link, pdf_link, objet, resume, last_updated)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (year, week, title, link, pdf_link, objet, resume, datetime.now()))
            added_count += 1

        conn.commit()

        if added_count > 0:
            st.success(f"{added_count} nouvelles instructions ont √©t√© ajout√©es !")
        else:
            st.info("Aucune nouvelle instruction trouv√©e.")

    except sqlite3.Error as e:
        st.error(f"Erreur SQLite : {e}")
    except Exception as e:
        st.error(f"Erreur inattendue : {e}")
    finally:
        if cursor:
            try:
                cursor.close()
                st.write("‚úÖ Connexion ferm√©e proprement.")
            except sqlite3.ProgrammingError:
                st.write("‚ö†Ô∏è Impossible de fermer le curseur, il est d√©j√† ferm√©.")
        if conn:
            try:
                conn.close()
                st.write("‚úÖ Connexion ferm√©e proprement.")
            except sqlite3.ProgrammingError:
                st.write("‚ö†Ô∏è La connexion √©tait d√©j√† ferm√©e.")

# Afficher les mises √† jour r√©centes
st.sidebar.header("Mises √† jour r√©centes")
if st.sidebar.button("Afficher les mises √† jour r√©centes"):
    if 'last_updated' not in data.columns:
        st.error("La colonne 'last_updated' est manquante dans la base de donn√©es.")
    else:
        recent_updates = data.sort_values(by='last_updated', ascending=False).head(10)
        st.write("Derni√®res mises √† jour :")
        st.dataframe(recent_updates[['title', 'link', 'pdf_link', 'objet', 'resume', 'last_updated']])

